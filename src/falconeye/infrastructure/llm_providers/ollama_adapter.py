"""Ollama LLM adapter implementation."""

import asyncio
from typing import List
import time
import ollama
from ...domain.services.llm_service import LLMService
from ...domain.models.prompt import PromptContext
from ..logging import FalconEyeLogger, logging_context


class OllamaLLMAdapter(LLMService):
    """
    Ollama LLM adapter for local AI-powered analysis.

    This adapter provides the PRIMARY mechanism for security analysis.
    ALL findings come from the LLM - NO pattern matching.

    Uses Ollama for:
    - Security analysis (qwen3-coder:30b)
    - Embeddings (embeddinggemma:300m)
    """

    def __init__(
        self,
        host: str = "http://localhost:11434",
        chat_model: str = "qwen3-coder:30b",
        embedding_model: str = "embeddinggemma:300m",
        chat_max_tokens: int = 256000,
        temperature: float = 0.0,
        max_response_tokens: int = 8192,
    ):
        """
        Initialize Ollama adapter.

        Args:
            host: Ollama server URL
            chat_model: Model for chat/analysis
            embedding_model: Model for embeddings
            chat_max_tokens: Context window size
            temperature: Sampling temperature (0.0 = deterministic)
            max_response_tokens: Max tokens in response
        """
        self.host = host
        self.chat_model = chat_model
        self.embedding_model = embedding_model
        self.chat_max_tokens = chat_max_tokens
        self.temperature = temperature
        self.max_response_tokens = max_response_tokens

        # Initialize Ollama client
        self.client = ollama.Client(host=host)

        # Initialize logger
        self.logger = FalconEyeLogger.get_instance()

    async def analyze_code_security(
        self,
        context: PromptContext,
        system_prompt: str,
    ) -> str:
        """
        Analyze code for security vulnerabilities using AI.

        This is the CORE method for security analysis.
        ALL security findings are generated by the LLM,
        with NO pattern matching whatsoever.

        Args:
            context: Full context for AI analysis
            system_prompt: System instructions for AI

        Returns:
            Raw AI response with security findings
        """
        with logging_context(operation="llm_analysis"):
            start_time = time.time()

            # Format context for AI
            user_prompt = context.format_for_ai()
            prompt_length = len(user_prompt)

            self.logger.info(
                "Starting security analysis",
                extra={
                    "model": self.chat_model,
                    "prompt_length": prompt_length,
                    "temperature": self.temperature
                }
            )

            try:
                # Call LLM
                response = await self._call_ollama(
                    system_prompt=system_prompt,
                    user_prompt=user_prompt,
                )

                duration = time.time() - start_time
                response_length = len(response) if response else 0

                self.logger.info(
                    "Security analysis completed",
                    extra={
                        "duration_seconds": round(duration, 2),
                        "prompt_length": prompt_length,
                        "response_length": response_length,
                        "model": self.chat_model
                    }
                )

                return response

            except Exception as e:
                duration = time.time() - start_time
                self.logger.error(
                    "Security analysis failed",
                    exc_info=True,
                    extra={
                        "duration_seconds": round(duration, 2),
                        "error_type": type(e).__name__,
                        "model": self.chat_model,
                        "host": self.host
                    }
                )
                raise

    async def generate_embedding(self, text: str) -> List[float]:
        """
        Generate embedding vector for text.

        Args:
            text: Text to embed

        Returns:
            Embedding vector
        """
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.client.embeddings(
                model=self.embedding_model,
                prompt=text,
            )
        )
        return response["embedding"]

    async def generate_embeddings_batch(
        self,
        texts: List[str],
    ) -> List[List[float]]:
        """
        Generate embeddings for multiple texts.

        Args:
            texts: List of texts to embed

        Returns:
            List of embedding vectors
        """
        with logging_context(operation="embedding_generation"):
            start_time = time.time()
            batch_size = len(texts)
            total_chars = sum(len(t) for t in texts)

            self.logger.info(
                "Starting batch embedding generation",
                extra={
                    "batch_size": batch_size,
                    "total_chars": total_chars,
                    "model": self.embedding_model
                }
            )

            try:
                # Process in parallel
                tasks = [self.generate_embedding(text) for text in texts]
                embeddings = await asyncio.gather(*tasks)

                duration = time.time() - start_time
                avg_time_per_embedding = duration / batch_size if batch_size > 0 else 0

                self.logger.info(
                    "Batch embedding generation completed",
                    extra={
                        "batch_size": batch_size,
                        "duration_seconds": round(duration, 2),
                        "avg_time_per_embedding": round(avg_time_per_embedding, 3),
                        "embeddings_per_second": round(batch_size / duration, 2) if duration > 0 else 0,
                        "model": self.embedding_model
                    }
                )

                return embeddings

            except Exception as e:
                duration = time.time() - start_time
                self.logger.error(
                    "Batch embedding generation failed",
                    exc_info=True,
                    extra={
                        "batch_size": batch_size,
                        "duration_seconds": round(duration, 2),
                        "error_type": type(e).__name__,
                        "model": self.embedding_model,
                        "host": self.host
                    }
                )
                raise

    async def validate_findings(
        self,
        code_snippet: str,
        findings: str,
        context: str,
    ) -> str:
        """
        Use AI to validate findings and remove false positives.

        The AI re-evaluates findings to ensure they're genuine.
        NO pattern-based filtering.

        Args:
            code_snippet: Original code
            findings: Initial findings (JSON)
            context: Additional context

        Returns:
            Validated findings (AI-filtered)
        """
        system_prompt = """You are a security expert validating security findings.
Review each finding carefully and determine if it's a genuine security issue.

Remove false positives by checking:
1. Is the vulnerability actually present in the code?
2. Are there mitigations already in place?
3. Is the severity assessment accurate?
4. Is the reasoning sound?

Return only the VALID findings in the same JSON format.
If all findings are false positives, return: {"reviews": []}
"""

        user_prompt = f"""CODE:
{code_snippet}

INITIAL FINDINGS:
{findings}

ADDITIONAL CONTEXT:
{context}

Validate these findings and return only genuine security issues."""

        response = await self._call_ollama(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        return response

    async def summarize_findings(
        self,
        findings: List[str],
    ) -> str:
        """
        Use AI to summarize multiple findings.

        Args:
            findings: List of finding descriptions

        Returns:
            Summary of findings
        """
        system_prompt = """You are a security expert summarizing security findings.
Create a concise but comprehensive summary of all identified issues."""

        user_prompt = f"""Summarize these security findings:

{chr(10).join(findings)}"""

        response = await self._call_ollama(
            system_prompt=system_prompt,
            user_prompt=user_prompt,
        )

        return response

    def count_tokens(self, text: str) -> int:
        """
        Estimate token count for text.

        This is a rough estimate. Actual tokenization may differ.

        Args:
            text: Text to count tokens for

        Returns:
            Estimated token count
        """
        # Rough estimate: ~4 characters per token
        return len(text) // 4

    async def health_check(self) -> bool:
        """
        Check if Ollama service is available.

        Returns:
            True if service is healthy
        """
        try:
            loop = asyncio.get_event_loop()
            response = await loop.run_in_executor(
                None,
                lambda: self.client.list()
            )

            # Check if required models are available
            model_names = [m.model for m in response.models]
            has_chat = any(self.chat_model in name for name in model_names)
            has_embedding = any(self.embedding_model in name for name in model_names)

            return has_chat and has_embedding

        except Exception as e:
            print(f"Ollama health check failed: {e}")
            return False

    async def _call_ollama(
        self,
        system_prompt: str,
        user_prompt: str,
    ) -> str:
        """
        Internal method to call Ollama API.

        Args:
            system_prompt: System instructions
            user_prompt: User query

        Returns:
            AI response text
        """
        messages = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ]

        # Run in executor to avoid blocking
        loop = asyncio.get_event_loop()
        response = await loop.run_in_executor(
            None,
            lambda: self.client.chat(
                model=self.chat_model,
                messages=messages,
                options={
                    "temperature": self.temperature,
                    "num_ctx": self.chat_max_tokens,
                    "num_predict": self.max_response_tokens,
                },
            )
        )

        return response["message"]["content"]